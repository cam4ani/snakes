{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic package\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import numpy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "#to set connection with Flickr API\n",
    "from flickrapi import FlickrAPI\n",
    "\n",
    "#image\n",
    "from PIL import Image\n",
    "\n",
    "#url open to get image\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "\n",
    "#date\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "#plot (for image verification)\n",
    "import cv2\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "path_data = os.path.join(ROOT_DIR,'snakes_data/SNAPP_images')\n",
    "path_ml = os.path.join(ROOT_DIR,'SNAKES')\n",
    "folder_path = os.path.join(path_ml,'datasets','flickr_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get these data fllow the direction of: http://joequery.me/code/flickr-api-image-search-python/\n",
    "FLICKR_PUBLIC = 'dd0cb0ced4e83452f8d49cb3d534707d'\n",
    "FLICKR_SECRET = '4c568d2002b5506e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr = FlickrAPI(FLICKR_PUBLIC, FLICKR_SECRET, format='parsed-json')\n",
    "extras = 'description,geo,tags,url_c,owner_name,date_taken,license'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create appropriate folder if not existing\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download images from Flickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case to erase all\n",
    "#shutil.rmtree(os.path.join(path_ml,'datasets','flickr_images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "#look into more details of the 'safe_search' parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFO:\n",
    "#for more parameter options: https://www.flickr.com/services/api/flickr.photos.search.html\n",
    "#tags (Optional): A comma-delimited list of tags. Photos with one or more (or all tags by changing tags_mode)of the \n",
    "#tags listed will be returned. You can exclude results that match a term by prepending it with a - character.\n",
    "#http://joequery.me/code/flickr-api-image-search-python/\n",
    "#lisence info: https://www.flickr.com/services/api/flickr.photos.licenses.getInfo.html\n",
    "#geolocalisation should not be used, as for example picture might be taken from a museum. we should add geolocalisation \n",
    "#based on 'biology' knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have in total 3700 species\n"
     ]
    }
   ],
   "source": [
    "#produce a list of species to search for\n",
    "li_species = []\n",
    "for genus in glob.glob(os.path.join(path_ml,'datasets','snapp_images','*')):\n",
    "    for species in glob.glob(os.path.join(genus,'*')):\n",
    "        li_species.append(species.split('/')[-1])\n",
    "print('We have in total %d species'%len(li_species))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create appropriate folder if needed\n",
    "for species in li_species:\n",
    "    folder_path_s = os.path.join(folder_path,species)\n",
    "    if not os.path.exists(folder_path_s):\n",
    "        os.makedirs(folder_path_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#li_species.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download images form flickr\n",
    "#Go in each species folder, and downlaod all the photos with a taken date greater than or equal to the maximum one \n",
    "#recorded in the species-metadata file if it exist, otherwise download it from the begining (\"0000-00-00 00:00:00\")\n",
    "#While downloading an image, if there is an error from flickr stop the code (might be connection error). Then you \n",
    "#simply need to rerun it perhaps few minutes later\n",
    "\n",
    "for species in tqdm.tqdm(li_species):\n",
    "                    \n",
    "    #initialization\n",
    "    t = \"0000-00-00 00:00:00\"\n",
    "    df_old = pd.DataFrame()\n",
    "    folder_path_s = os.path.join(folder_path,species)\n",
    "\n",
    "    #if the new collection of images is empty, then stop it, otherwise continue with the last taken date\n",
    "    while True:\n",
    "\n",
    "        #open the existing metadata file if any, and use the max taken date to grab data from that point instead\n",
    "        old_meta_data_file = os.path.join(folder_path_s,'flickr_df_'+species+'.csv')\n",
    "        if len(glob.glob(old_meta_data_file))>0:\n",
    "            #date will be: Timestamp('2017-04-30 17:12:52')\n",
    "            df_old = pd.read_csv(old_meta_data_file, parse_dates=['datetaken'], index_col=False, sep=';')\n",
    "            #we need to take minus one day as min_taken_date is apparently working at day level, and before saving\n",
    "            #we'll need to remove possibly duplicates (might happen if several picture taken the same day but we stop\n",
    "            #at a \"middle picture of the day\"). Also we convert to good format for flickr query\n",
    "            t = (max(df_old['datetaken'].tolist()) - dt.timedelta(days=1)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        #print()#to debug\n",
    "        #print(t)#to debug\n",
    "        #print('lets collect %s'%species)#to debug\n",
    "        try:\n",
    "            image_data = flickr.photos.search(text='\\\"'+species+'\\\"', content_type=1, media=\"photos\", \n",
    "                                              per_page=500, extras=extras, min_taken_date=t)\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Not able to DOWNLOAD flickr image for species %s'%species)\n",
    "            #TODO: investigate how to avoid these exception\n",
    "            sys.exit()\n",
    "\n",
    "        #download image with its url and save it\n",
    "        for i, photo in enumerate(image_data['photos']['photo']): #besides photos there is only a 'stat' key\n",
    "            if 'url_c' in photo:\n",
    "                url = photo['url_c']\n",
    "                img = Image.open(urlopen(url))\n",
    "                try:\n",
    "                    img.save(os.path.join(folder_path_s,species+'_'+photo['id']+\".png\"))\n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print('Not able to SAVE flickr image for species %s and url %s'%(species,str(url)))\n",
    "                del img\n",
    "\n",
    "        #create new metadata file with all the images (old and new)\n",
    "        df_new = pd.DataFrame(image_data['photos']['photo'])\n",
    "        df = pd.concat([df_old, df_new], ignore_index=True)\n",
    "\n",
    "        #save and remove duplicates (before: uniform the id type(as when we save and open the str get converted to int))\n",
    "        if df.shape[0]>0:\n",
    "            df['id'] = df['id'].astype(int) \n",
    "            df = df.drop_duplicates(subset=['id'], keep='first', inplace=False)\n",
    "            #save metadata for each images of this species (note: might be empty if no images was collected)\n",
    "            df.to_csv(os.path.join(folder_path_s,'flickr_df_'+species+'.csv'), index=False, sep=';')\n",
    "\n",
    "        #print(df_old.shape,df_new.shape,df.shape) #to debug\n",
    "        #if there was already data collected and the new one brought some more data\n",
    "        if (df_old.shape[0]>0) and (df.shape[0]>df_old.shape[0]):\n",
    "            print('species %s needed two collected data'%species)\n",
    "        #if no more data was bring last time\n",
    "        if df.shape[0]==df_old.shape[0]: #wrong: df_new.shape[0]==0: indeed we can gather images that are already (-1d)\n",
    "            break\n",
    "\n",
    "#do_request: Status code 502 received\n",
    "#('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
    "#do_request: Status code 500 received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#species Ithycyphus miniatus needed two collected data\n",
    "#species Ithycyphus oursi needed two collected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check amount of flickr images we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 35777 images collected from Flickr\n"
     ]
    }
   ],
   "source": [
    "#keeping meta data of only the images we truely have \n",
    "#look at the actual image we really have\n",
    "li_flickr_images = []\n",
    "for species in glob.glob(os.path.join(path_ml,'datasets','flickr_images','*')):\n",
    "    li_flickr_images.extend([x for x in glob.glob(os.path.join(species,'*')) if not x.endswith('.csv')])\n",
    "len(li_flickr_images)\n",
    "print('We have %d images collected from Flickr'%len(li_flickr_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create one csv file with all metadata info from each species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "li_df = []\n",
    "for species in tqdm.tqdm(glob.glob(os.path.join(path_ml,'datasets','flickr_images','*'))):\n",
    "    csv_f = glob.glob(os.path.join(species,'*.csv'))\n",
    "    if len(csv_f)==1:\n",
    "        df = pd.read_csv(csv_f[0], sep=';', index_col=False)\n",
    "        df['species'] = species.split('/')[-1]\n",
    "        li_df.append(df)\n",
    "        del df\n",
    "df_all = pd.concat(li_df, ignore_index=True)\n",
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49977, 31)\n",
      "(35777, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>context</th>\n",
       "      <th>datetaken</th>\n",
       "      <th>datetakengranularity</th>\n",
       "      <th>datetakenunknown</th>\n",
       "      <th>description</th>\n",
       "      <th>farm</th>\n",
       "      <th>geo_is_contact</th>\n",
       "      <th>geo_is_family</th>\n",
       "      <th>geo_is_friend</th>\n",
       "      <th>...</th>\n",
       "      <th>place_id</th>\n",
       "      <th>secret</th>\n",
       "      <th>server</th>\n",
       "      <th>species</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>url_c</th>\n",
       "      <th>width_c</th>\n",
       "      <th>woeid</th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-10-04 03:30:32</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'_content': ''}</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a8bfa9d292</td>\n",
       "      <td>1922</td>\n",
       "      <td>Acanthophis praelongus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Northern death adder from Cairns. Acanthophis ...</td>\n",
       "      <td>https://farm2.staticflickr.com/1922/4331268958...</td>\n",
       "      <td>800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/mount/SDB/camille-secure/SNAKES/datasets/flic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-10-04 03:30:43</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'_content': ''}</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5b032f28ec</td>\n",
       "      <td>1954</td>\n",
       "      <td>Acanthophis praelongus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Northern death adder from Cairns. Acanthophis ...</td>\n",
       "      <td>https://farm2.staticflickr.com/1954/3125232722...</td>\n",
       "      <td>800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/mount/SDB/camille-secure/SNAKES/datasets/flic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-10-03 19:39:42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'_content': 'Mt Molloy - Far North Queensland...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9893fd3e82</td>\n",
       "      <td>1925</td>\n",
       "      <td>Acanthophis praelongus</td>\n",
       "      <td>northern death adder acanthophis praelongus sh...</td>\n",
       "      <td>Northern Death Adder (Acanthophis praelongus)</td>\n",
       "      <td>https://farm2.staticflickr.com/1925/4326216653...</td>\n",
       "      <td>800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/mount/SDB/camille-secure/SNAKES/datasets/flic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  context            datetaken  datetakengranularity  \\\n",
       "0         0        0  2018-10-04 03:30:32                     0   \n",
       "1         0        0  2018-10-04 03:30:43                     0   \n",
       "2         0        0  2018-10-03 19:39:42                     0   \n",
       "\n",
       "   datetakenunknown                                        description  farm  \\\n",
       "0                 1                                   {'_content': ''}     2   \n",
       "1                 1                                   {'_content': ''}     2   \n",
       "2                 0  {'_content': 'Mt Molloy - Far North Queensland...     2   \n",
       "\n",
       "   geo_is_contact  geo_is_family  geo_is_friend  \\\n",
       "0             NaN            NaN            NaN   \n",
       "1             NaN            NaN            NaN   \n",
       "2             NaN            NaN            NaN   \n",
       "\n",
       "                         ...                          place_id      secret  \\\n",
       "0                        ...                               NaN  a8bfa9d292   \n",
       "1                        ...                               NaN  5b032f28ec   \n",
       "2                        ...                               NaN  9893fd3e82   \n",
       "\n",
       "   server                 species  \\\n",
       "0    1922  Acanthophis praelongus   \n",
       "1    1954  Acanthophis praelongus   \n",
       "2    1925  Acanthophis praelongus   \n",
       "\n",
       "                                                tags  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  northern death adder acanthophis praelongus sh...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Northern death adder from Cairns. Acanthophis ...   \n",
       "1  Northern death adder from Cairns. Acanthophis ...   \n",
       "2      Northern Death Adder (Acanthophis praelongus)   \n",
       "\n",
       "                                               url_c  width_c  woeid  \\\n",
       "0  https://farm2.staticflickr.com/1922/4331268958...    800.0    NaN   \n",
       "1  https://farm2.staticflickr.com/1954/3125232722...    800.0    NaN   \n",
       "2  https://farm2.staticflickr.com/1925/4326216653...    800.0    NaN   \n",
       "\n",
       "                                            img_path  \n",
       "0  /mount/SDB/camille-secure/SNAKES/datasets/flic...  \n",
       "1  /mount/SDB/camille-secure/SNAKES/datasets/flic...  \n",
       "2  /mount/SDB/camille-secure/SNAKES/datasets/flic...  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['img_path'] = df_all.apply(lambda x: os.path.join(folder_path,x['species'], \n",
    "                                                         x['species']+'_'+str(x['id'])+\".png\"), axis=1)\n",
    "print(df_all.shape)\n",
    "#keeping only the images we truely have \n",
    "df_all = df_all[df_all['img_path'].isin(li_flickr_images)]\n",
    "print(df_all.shape)\n",
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save metadata info (might not be of same size of number of collected images)\n",
    "df_all.to_csv(os.path.join(folder_path,'flickr_image_info.csv'),index=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
